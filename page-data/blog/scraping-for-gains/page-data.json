{"componentChunkName":"component---src-js-templates-blog-template-js","path":"/blog/scraping-for-gains","result":{"data":{"markdownRemark":{"html":"<h1>The Challenge</h1>\n<p>I was challenged, by none other than my exceedingly lovely girlfriend, to go (almost) vegetarian for the month of April. 6 days a week for the whole month, I am to eat nothing but the food that my food eats.</p>\n<p>In all honesty, this is really good for me. I would like to think of myself as both humane and environmentally conscious, both self-images that are pretty heavily distorted by relentless daily meat consumption. I came into the challenge hoping that it would help resolve some of that cognitive dissonance, and it surely has. There are myriad other benefits to a fruit- and veggie- heavy diet, but that's the main one for me.</p>\n<p>There's just one problem: plants aren't quite as good at packing ridiculous amounts of protein per calorie as, say, chicken breast or pork tenderloin, and I would also like to think of myself as somewhat of a strong person. At the very least, I'm in the practice of regularly lifting heavy weights and putting them back down, and I've been told that necessitates a higher than average protein intake.</p>\n<h1>The Source</h1>\n<p>Luckily for me, my exceedingly lovely girlfriend recommended me a great repository of recipes to start my journey, <a href=\"https://www.budgetbytes.com\">Budget Bytes</a>. I think she likes it because she is a meticulous person, and this blog is written meticulously. I am also a meticulous person, and my favorite thing about Budget Bytes is its unwavering commitment to listing the macronutrient profiles of every recipe.</p>\n<p>// TODO: INSERT RECIPE PICTURE</p>\n<p>This, of course, means that I can judge each recipe primarily on how much protein it packs. Just one problem: There are <em>hundreds</em> of vegetarian recipes on this site, and no option to sort by protein ratio. As much as I like spending hours trawling through webpages and recording recipes manually, I like spending hours doing other stuff more. Like making my code trawl through webpages and record recipes <strong>automatically</strong>.</p>\n<p>It was time to get <a href=\"https://scrapy.org\">Scrap(p)y</a>.</p>\n<h1>We Could Rule This Website, Spider Man</h1>\n<p>There are lots of web scraping libraries for Python. I chose Scrapy because I've used it before in school and I quite liked it, but I'm sure they're all great. Scrapy is easy to install if you have Python 3 and pip. Also if you don't care about virtualizing your Python projects to isolate dependencies (which you should, and can learn how to <a href=\"https://pipenv.pypa.io/en/latest/\">here</a>, but I'm lazy so I didn't).</p>\n<pre><code class=\"language-language=bash\">% pip install scrapy\n</code></pre>\n<p>That done, let's navigate to whatever folder we want to hold our scraper and start the Scrapy project.</p>\n<pre><code class=\"language-language=bash\">% scrapy startproject scraping-for-gains\n</code></pre>\n<p>This is not a Scrapy tutorial; the devs themselves <a href=\"https://docs.scrapy.org/en/latest/intro/tutorial.html\">have done that much better than I could ever hope to</a>. This is a story about the compulsion to automation that Scrapy enables.</p>\n<p>That being said, the jist of Scrapy is this: the framework exposes a class, <code>Spider</code>, that knows how to crawl through a webpage and extract all the html. You tell it what pages to crawl by defining a list of URL strings in the <code>Spider.start_requests()</code> method and calling <code>scrapy.Request(url:callback:)</code> for each URL.</p>\n<p>Once the spider has retrieved all the html on the requested page, it passes the html to your callback function. By default this is <code>Spider.parse(response:)</code>. It can search for and return relevant elements using <a href=\"https://en.wikipedia.org/wiki/XPath\">XPaths</a> which target CSS selectors. Once you have the elements you want, Bob's your uncle. You can manipulate it however you can normally manipulate strings in Python!</p>\n<p>Another cute thing about Scrapy is the ability to define Items which let you serialize the data you collect into a POPO (Plain Ol' Python Object). For my case, I could just use a dictionary but what the heck. Here's my RecipeItem:</p>\n<pre><code class=\"language-language=python\">class RecipeItem(scrapy.Item):\n  url = scrapy.Field()\n  Serving = scrapy.Field()\n  Calories = scrapy.Field()\n  Carbohydrates = scrapy.Field()\n  Protein = scrapy.Field()\n  Fat = scrapy.Field()\n  Sodium = scrapy.Field()\n  Fiber = scrapy.Field()\n  ProteinRatio = scrapy.Field()\n</code></pre>\n<p>And here's my RecipesSpider:</p>\n<pre><code class=\"language-language=python\">import scrapy\nfrom ..items import RecipeItem\nfrom math import floor  \n\nclass RecipesSpider(scrapy.Spider):\n  name = \"recipes\"\n  recipes = []\n\n  def start_requests(self):\n    urls = [\n        'https://www.budgetbytes.com/category/recipes/vegetarian/',\n        'https://www.budgetbytes.com/category/recipes/vegetarian/page/2',\n        'https://www.budgetbytes.com/category/recipes/vegetarian/page/3',\n        'https://www.budgetbytes.com/category/recipes/vegetarian/page/4',\n        'https://www.budgetbytes.com/category/recipes/vegetarian/page/5',\n        'https://www.budgetbytes.com/category/recipes/vegetarian/page/6',\n    ]\n\n    for url in urls:\n      yield scrapy.Request(url=url, callback=self.parse_index)\n\n  def parse_index(self, response):\n    # Note: the Scrapy tutorial provides a way to follow all the links in 1 line.\n    # I chose to do it this way bc it's more verbose and imo easier to follow.\n    links = response.css('.more-link::attr(href)').getall()\n    for link in links:\n      yield response.follow(link, callback=self.parse_recipe)\n\n  def parse_recipe(self, response):\n    recipe = RecipeItem()\n    recipe[\"url\"] = response.url\n\n    # Each label holds the information for 1 nutritional dimension\n    # (serving size, carbs, protein, etc)\n    labels = response.css('.wprm-nutrition-label-text-nutrition-container')\n    for label in labels:\n      # Assumption: There are exactly 3 spans in every label. \n      components = label.css('span::text').getall()\n      dimensionName = components[0]\n      amount = components[1]\n      unit = components[2]\n\n      # Assumption: Every dimension name is followed by a colon and a space ': ',\n      # and I want to get rid of those.\n      recipe[dimensionName[:-2]] = amount\n\n    recipe[\"ProteinRatio\"] = float(recipe[\"Protein\"]) / float(recipe[\"Calories\"])\n    self.recipes.append(recipe)\n</code></pre>\n<p>Some notes on the above:</p>\n<p>Because I wanted to parse <em>all</em> the recipes, I did a two-step process. First I crawled the index pages and gathered the URLs for each individual recipe, then I followed each of those URLs for my main parsing. I'm lucky there were only 6 pages, otherwise I would be well-served to have a process for collecting every page in the index automatically.</p>\n<p>Once I got the recipe page, I packed the relevant information into my <code>RecipeItem</code>. For this I leaned heavily on some key assumptions about each recipe's markup. Thankfully, since the site author is meticulous, these assumptions were broadly founded. I did a little finagling to get things pretty, by slicing out the last 2 characters of each dimension name to turn \"Calories: \" into \"Calories\". Then I added the protein ratio, which I just defined as protein per calorie.</p>\n<p>Okay great. Now I have a bunch of data - what do I do with it? My first instinct was to try to just append to an output file in <code>parse_recipe()</code>. Tried that; no dice. Because of how <code>yield</code> works in Python, you can't guarantee the order of any of the calls, so the file gets all messed up.</p>\n<p>Good for us, the Scrapy devs have thought of this and given us Pipelines! We can create a Pipeline object and define the <code>close_spider(spider:)</code> method, which is run on a callback once the Spider finishes doing it's thing, and can access all of the Spider's stored state. From there, I grabbed all the recipe objects, sorted them, and printed them out:</p>\n<pre><code class=\"language-language=python\">class BudgetBulkyBytesPipeline:\n  def close_spider(self, spider):\n    sortedRecipes = sorted(spider.recipes, key=lambda recipe: recipe[\"ProteinRatio\"], reverse=True)\n\n    # TODO: Sum the carbs, fat, and protein and compare with calories to look for malformed recipes\n    # Protein: 4kcal, carbs: 4kcal, fat: 9kcal\n\n    # TODO: Email the author with a list of errata\n\n    print(sortedRecipes)\n    print(sortedRecipes.__len__())\n</code></pre>\n<h1>Too Good to be True</h1>","frontmatter":{"date":"April 25, 2021","path":"/blog/scraping-for-gains","title":"Scraping for Gains"}}},"pageContext":{}},"staticQueryHashes":[]}